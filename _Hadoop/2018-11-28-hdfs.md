---
layout : default
title : HDFS
excerpt: This is all about the hdfs 
permalink: /hdfs
---
{% include home_index.html%}
### Overwriting hdfs-site.xml
   
   - Using -D option in command, default configs can be over written
   - Below command overwrite default replication factor in cluster and writes data with replication factor 1.

      hdfs dfs -Ddfs.replication=1 -put /path/to/local/file /path/to/hdfs/dir


### Data Node went to Zombie state

   - Oneday moring i was randomly checking the cluster free space, as i scheduled one cleanup process previous day. I came to know that one of the DATANODE is not able to connect to namenode.
   
   - I checked DATANODE logs nothing is suspecious, so i went to Cloudera Manager Interface and restared DATANODE.(the problem started here!)
  
   - CM throwed timeout error but when i checked DATANODE logs, it clearly said DATANODE is shutdown.
  
   - I tried restarting cloudera-scm-agent on the box but no luck.

		sudo /etc/init.d/cloudera-scm-agent restart

   - cloudera-scm-agent has supervisord, that manages services start,stop,restart operations. When i checked its log it was continously printing two things

      - SIGTERM on DATANODE and 
      - DATANODE is already running.
  
   - One of my collegue suggested to do clean-restart scm agent on the box. [Link](https://www.cloudera.com/documentation/enterprise/5-14-x/topics/cm_ag_agents.html)
  
   - This gave us the clue that DATANODE port is already in use and DATANODE is running as Zombie process. Since it's Parent processes is init we have no option other than rebooting the server.
  
   - **Rebooting server worked fine.** 


### Name Node , Heap, Number of blocks, FSImage size

   - Oneday afternoon we all planned to go out to have lunch. You know these services somehow know the right panic situation to fail.

   -  One of our collegue called us saying that Hdfs Name Node is not coming up and it is stuck in safe mode by continuously throwing OOM.

   - After checking logs we decided to increase the heap size of Name Node. We increased it by 40% and Name node was up with out any issues.

   - To know the root cause for this, we analyzed our monitoring graphs to find out what could have happend.

   - Finally we came to know that one of recently on boarded team puts so many small files that increased number of blocks in our cluster exponentially.

   - This increased FSImage size which caused NameNode to stuck in safe Mode.

   - After couple of months we again faced so many GC pauses on namenode, This time with out incrasing NameNode Heap we dropped many unwanted files which helped us to avoid this.

   - **Small files problem** 
   			
   		- If there are so many small files which are smaller than HDFS block size they all create one entry for each file in FSImage which causes Huge FSImage file that in tern causes Load on NameNode Heap.

   		- Each entry for files,dir,blocks meta occupy 150B in namenode meta store(FSImage)

   		- [Cloudera - Small files Problem](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/)

   		- https://stackoverflow.com/questions/44967519/hdfs-and-small-files-part-2?noredirect=1&lq=1


### Automatic Failover in hdfs

   - Two important components 

       - org.apache.hadoop.ha.HealthMonitor

          - It periodically checks the health status of namenode by polling 8020 port

       - org.apache.hadoop.ha.ZKFailoverController
      
###  Quorum based HA NameNode set up


   - Standby namenode also performs checkpoints of the namespace state and thus it is not necessary to run a Secondary Namenode, Checkpoint Node, or Backup Nodes in HA cluster. 

### DataNode --> NameNode can we become Friends


### Heterogeneous Storage Type in HDFS

   -  HDFS provides following storage types 

		- RAM_DISK (Data Node Memory)
		- SSD (Solid State Drive/ low latency access like HBase)
		- DISK( To store data that need to be processed, defalut storage type)
		- ARCHIVE (High storage density, low processing resources)
   - you can attach storage mount points to these storage type. 

   - Data placement on these storage types depends on storage policies.

   - List all storage policies 

```
		
		 hdfs storagepolicies -listPolicies
		 Block Storage Policies:
			BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}
			BlockStoragePolicy{WARM:5, storageTypes=[DISK, ARCHIVE], creationFallbacks=[DISK, ARCHIVE], replicationFallbacks=[DISK, ARCHIVE]}
			BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
			BlockStoragePolicy{ONE_SSD:10, storageTypes=[SSD, DISK], creationFallbacks=[SSD, DISK], replicationFallbacks=[SSD, DISK]}
			BlockStoragePolicy{ALL_SSD:12, storageTypes=[SSD], creationFallbacks=[DISK], replicationFallbacks=[DISK]}
			BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}
```

   -  Use Hdfs **MOVER** to mover data from one storage type to onother

```
		hdfs dfs mover [-f <local file that contains list of hdfs paths> -p <files/dirs>]
```
### Important Data structures in hdfs


### What the f*_*k is Non-DFS Used

```
		Non DFS Used = Configured Capacity - Remaining Space - DFS Used

		Configured Capacity = Total Disk Space - Reserved Space

```

   - ** DataSpill**  in MapReduce

   - ** DataSpill ** in spark 

### Namespace and Meta data in hdfs

   - **Namespace** : It is an abstraction over set of names. In hdfs it is set of hdfs paths(dir, files).
   - Meta Data : Name Node stores meta data in two files **FSImage** and **EditLogs**

### Important hdfs configurations



